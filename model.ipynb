{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten,BatchNormalization\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we will load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x_train (60000, 28, 28)\n",
      "Dimension of x_test (10000, 28, 28)\n",
      "Dimension of y_test (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Now we will se our dataset\n",
    "\n",
    "print(\"Dimension of x_train\",x_train.shape)\n",
    "print(\"Dimension of x_test\",x_test.shape)\n",
    "print(\"Dimension of y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOJklEQVR4nO3dbawc5XnG8evC2AYMaW0olguGkGAgNKUmPQIaUAvipQSpMeQF4VSRK5E6IEhDFdRSqgo+UAm1EERRmuAEy6alkFQEYTW0xLgIlKpxOCADBgdMkB3sGpsXgU0p9vHh7oczjg5w5tnj3dkXc/9/0tHuzr2zc2vlyzM7z84+jggB+PDbr98NAOgNwg4kQdiBJAg7kARhB5LYv5cbm+bpcYBm9HKTQCrv6H+1K3Z6olpHYbd9vqRbJU2R9L2IuLH0/AM0Q6f67E42CaBgdayqrbV9GG97iqRvSfqMpBMlLbR9YruvB6C7OvnMfoqkFyLixYjYJekeSQuaaQtA0zoJ+xGSXhr3eFO17D1sL7Y9bHt4RDs72ByATnT9bHxELImIoYgYmqrp3d4cgBqdhH2zpLnjHh9ZLQMwgDoJ+2OS5tk+xvY0SZdIWtFMWwCa1vbQW0Tstn2lpAc1NvS2NCKeaawzAI3qaJw9Ih6Q9EBDvQDoIr4uCyRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiioymbbW+QtEPSqKTdETHURFMAmtdR2CtnRcSrDbwOgC7iMB5IotOwh6Qf237c9uKJnmB7se1h28Mj2tnh5gC0q9PD+DMiYrPtwyWttP3ziHh0/BMiYomkJZL0Ec+KDrcHoE0d7dkjYnN1u03SfZJOaaIpAM1rO+y2Z9g+ZM99SedJWttUYwCa1clh/GxJ99ne8zr/EhH/0UhXABrXdtgj4kVJv9NgLwC6iKE3IAnCDiRB2IEkCDuQBGEHkmjiQhgMsF1/WL4QceMfv1usX/6pR4r1q2Y+v9c97fHb3/tasX7QlvIXLt/4dPnr10ffVb8vm/bgcHHdDyP27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsHwKvXPZ7tbXb/uJbxXWHpo8W6/u12B8s2nBOsX7yr/2ytvbkV24trttKq94+PWthbW3Wgx1tep/Enh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcfQB46rRi/Z1zyj/ie+9f/X1t7Tf3n15c99KN5xbrG286vlif8aM1xfrDBx1VW3vkvuOK6947b0Wx3sr2NYfW1mZ19Mr7JvbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wDYMuV5d92/9nVra77rh9L/+ILf1Rcc/fnR4r1g15dXayXf9ld+p/Fv1tbWz2vs+vZ//3tQ4r1Y29/qba2u6Mt75ta7tltL7W9zfbacctm2V5pe311O7O7bQLo1GQO45dJOv99y66RtCoi5klaVT0GMMBahj0iHpX0+vsWL5C0vLq/XNKFDfcFoGHtfmafHRFbqvsvS5pd90TbiyUtlqQDdFCbmwPQqY7PxkdEqHCeJiKWRMRQRAxNLZxIAtBd7YZ9q+05klTdbmuuJQDd0G7YV0haVN1fJOn+ZtoB0C0tP7PbvlvSmZIOs71J0nWSbpT0A9uXStoo6eJuNrmvW3/bqcX6c5+7rVgvz6AufWLlZbW1E67eUFx39NXXWrx6Zy67vHv7gRv+dlGxPvOl/+7atvdFLcMeEXW/tH92w70A6CK+LgskQdiBJAg7kARhB5Ig7EASXOLagF/cfFqx/tznytMmv/nuO8X6F3/+pWL9+K89X1sb3bGjuG4r+82YUay/9oWTivUFB9f/zPV+OrC47gn/ekWxfuwyhtb2Bnt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfZJmjL78Nra8ov+sbjuuy0uUm01jj7t3I0tXr99+80/sVj/5NJ1xfoNs/+hxRbqf53o9DWXFNc8/vrytkdbbBnvxZ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnH2SfED9ePHQ9M5GfA/8s2nlbR89t1hff9mRtbXzznmiuO6fH76kWD9q//I1563G+EejflJnf/+w8rpvrG/x6tgb7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2Scp3tlZW1u9c2px3VOnjxTr9z90T7He6nr4Tjz0f+Wx7vUj9ePkknTWgW8V68O76r9D8Ot38rvvvdRyz257qe1ttteOW3a97c2211R/F3S3TQCdmsxh/DJJ50+w/JaImF/9PdBsWwCa1jLsEfGopNd70AuALurkBN2Vtp+qDvNn1j3J9mLbw7aHR1T/uRdAd7Ub9m9L+rik+ZK2SLq57okRsSQihiJiaGrhxwcBdFdbYY+IrRExGhHvSvqupFOabQtA09oKu+054x5eJGlt3XMBDIaW4+y275Z0pqTDbG+SdJ2kM23PlxSSNkj6ahd7HAijW7fV1q67/CvFdW/6Tvl35U8qX86uf95evp79hkc+W1s7bll57vf9t75ZrB9+d/nc7Flz/7NYX/Rw/XtznIaL66JZLcMeEQsnWHxHF3oB0EV8XRZIgrADSRB2IAnCDiRB2IEkuMS1AdMeLA8hXXtMd79zdJx+1va6OxaUe/vRUfcX6yNR3l8cuKHFuCJ6hj07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBOHtyuw8s/38/EuXpqFv9zPUxy35Zv+3immgae3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9uQOueen5SfUzvWDfQ17diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH25HZcclqLZzzekz7QfS337Lbn2n7Y9rO2n7H99Wr5LNsrba+vbmd2v10A7ZrMYfxuSd+IiBMlnSbpCtsnSrpG0qqImCdpVfUYwIBqGfaI2BIRT1T3d0haJ+kISQskLa+etlzShd1qEkDn9uozu+2PSjpZ0mpJsyNiS1V6WdLsmnUWS1osSQfooHb7BNChSZ+Nt32wpHslXRUR28fXIiIkxUTrRcSSiBiKiKGpmt5RswDaN6mw256qsaDfFRE/rBZvtT2nqs+RtK07LQJoQsvDeNuWdIekdRHxzXGlFZIWSbqxui3P7YuB9ObH+KpFFpP5zH66pC9Letr2mmrZtRoL+Q9sXyppo6SLu9MigCa0DHtE/ESSa8pnN9sOgG7hGA5IgrADSRB2IAnCDiRB2IEkuMQ1uSMeebtYn3rllGJ9ZMLvTWIQsWcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ0/O/7WmWF+2/fBifeEhm4v1t39rTm1t2kubiuuiWezZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlRdMvtXyjWF159a7E+529eqK299sZJ5Y3/9KlyHXuFPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJOGI8g9/254r6U5JsyWFpCURcavt6yX9qaRXqqdeGxEPlF7rI54Vp5qJX/clUw47tFifdm/5qxrfP/bfamt/8OTC4rqzvvRKsT76xpvFekarY5W2x+sTzro8mS/V7Jb0jYh4wvYhkh63vbKq3RIRNzXVKIDumcz87Fskbanu77C9TtIR3W4MQLP26jO77Y9KOlnS6mrRlbafsr3U9syadRbbHrY9PKKdHTULoH2TDrvtgyXdK+mqiNgu6duSPi5pvsb2/DdPtF5ELImIoYgYmqrpDbQMoB2TCrvtqRoL+l0R8UNJioitETEaEe9K+q6kU7rXJoBOtQy7bUu6Q9K6iPjmuOXjfzb0Iklrm28PQFMmczb+dElflvS07T2/O3ytpIW252tsOG6DpK92pUP01eirrxXruz5fHpr7xM31/yzWnXN7cd3PnnBpsc4lsHtnMmfjfyJponG74pg6gMHCN+iAJAg7kARhB5Ig7EAShB1IgrADSbS8xLVJXOIKdFfpElf27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRE/H2W2/ImnjuEWHSXq1Zw3snUHtbVD7kuitXU32dnRE/MZEhZ6G/QMbt4cjYqhvDRQMam+D2pdEb+3qVW8cxgNJEHYgiX6HfUmft18yqL0Nal8SvbWrJ7319TM7gN7p954dQI8QdiCJvoTd9vm2n7P9gu1r+tFDHdsbbD9te43t4T73stT2Nttrxy2bZXul7fXV7YRz7PWpt+ttb67euzW2L+hTb3NtP2z7WdvP2P56tbyv712hr568bz3/zG57iqTnJZ0raZOkxyQtjIhne9pIDdsbJA1FRN+/gGH79yW9JenOiPhktezvJL0eETdW/1HOjIi/HJDerpf0Vr+n8a5mK5ozfppxSRdK+hP18b0r9HWxevC+9WPPfoqkFyLixYjYJekeSQv60MfAi4hHJb3+vsULJC2v7i/X2D+WnqvpbSBExJaIeKK6v0PSnmnG+/reFfrqiX6E/QhJL417vEmDNd97SPqx7cdtL+53MxOYHRFbqvsvS5rdz2Ym0HIa71563zTjA/PetTP9eac4QfdBZ0TEpyR9RtIV1eHqQIqxz2CDNHY6qWm8e2WCacZ/pZ/vXbvTn3eqH2HfLGnuuMdHVssGQkRsrm63SbpPgzcV9dY9M+hWt9v63M+vDNI03hNNM64BeO/6Of15P8L+mKR5to+xPU3SJZJW9KGPD7A9ozpxItszJJ2nwZuKeoWkRdX9RZLu72Mv7zEo03jXTTOuPr93fZ/+PCJ6/ifpAo2dkf+FpL/uRw81fX1M0pPV3zP97k3S3Ro7rBvR2LmNSyUdKmmVpPWSHpI0a4B6+ydJT0t6SmPBmtOn3s7Q2CH6U5LWVH8X9Pu9K/TVk/eNr8sCSXCCDkiCsANJEHYgCcIOJEHYgSQIO5AEYQeS+H+ctitrvLo9awAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#we will see a single image in out dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[4])\n",
    "plt.show()\n",
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now defining some parameters for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "#as we have 10 classes (0-9) \n",
    "#we need to prdeict one out of 10 which has high probability\n",
    "epochs = 30\n",
    "img_rows = 28 \n",
    "img_cols = 28\n",
    "#as each image is 28 by 28 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing image \n",
    "x_train =x_train.astype(float)\n",
    "x_train =x_train/255\n",
    "x_test =x_test.astype(float)\n",
    "x_test =x_test/255\n",
    "#y_train =y_train/255\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes = 10, dtype = 'float32')\n",
    "y_test = to_categorical(y_test, num_classes = 10, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format in which image are feed to our model \n",
    "#there are two cases possible either we can have channel first than image dimension\n",
    "#or we can have dimension first than channel \n",
    "#we have one channel as it is a grey scale image therfore '1'\n",
    "if K.image_data_format() =='channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0],1,img_rows,img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0],1,img_rows,img_cols)\n",
    "    input_shape = (1,img_rows,img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0],img_rows,img_cols,1)\n",
    "    x_test = x_test.reshape(x_test.shape[0],img_rows,img_cols,1)\n",
    "    input_shape = (img_rows,img_cols,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build our model\n",
    "#BY DEFAULT STRIDE IS 1\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = input_shape))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 11, 11, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 32)          25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 6, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 1, 1, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 1, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 194,762\n",
      "Trainable params: 194,250\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 38s 638us/sample - loss: 0.4194 - accuracy: 0.8753 - val_loss: 0.7520 - val_accuracy: 0.7056\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 42s 695us/sample - loss: 0.1178 - accuracy: 0.9689 - val_loss: 0.0510 - val_accuracy: 0.9827\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 35s 579us/sample - loss: 0.0803 - accuracy: 0.9780 - val_loss: 0.0377 - val_accuracy: 0.9872\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 39s 642us/sample - loss: 0.0612 - accuracy: 0.9832 - val_loss: 0.0392 - val_accuracy: 0.9889\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 44s 728us/sample - loss: 0.0526 - accuracy: 0.9855 - val_loss: 0.0264 - val_accuracy: 0.9912\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 43s 709us/sample - loss: 0.0457 - accuracy: 0.9875 - val_loss: 0.0266 - val_accuracy: 0.9911\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 41s 680us/sample - loss: 0.0395 - accuracy: 0.9894 - val_loss: 0.0303 - val_accuracy: 0.9910\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 36s 599us/sample - loss: 0.0361 - accuracy: 0.9902 - val_loss: 0.0411 - val_accuracy: 0.9889\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 35s 592us/sample - loss: 0.0329 - accuracy: 0.9908 - val_loss: 0.0306 - val_accuracy: 0.9905\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 40s 668us/sample - loss: 0.0290 - accuracy: 0.9916 - val_loss: 0.0260 - val_accuracy: 0.9921\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 51s 851us/sample - loss: 0.0306 - accuracy: 0.9911 - val_loss: 0.0310 - val_accuracy: 0.9916\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 58s 965us/sample - loss: 0.0268 - accuracy: 0.9926 - val_loss: 0.0265 - val_accuracy: 0.9935\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 41s 683us/sample - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.0232 - val_accuracy: 0.9935\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 40s 663us/sample - loss: 0.0247 - accuracy: 0.9930 - val_loss: 0.0248 - val_accuracy: 0.9931\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 38s 641us/sample - loss: 0.0225 - accuracy: 0.9937 - val_loss: 0.0282 - val_accuracy: 0.9922\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 40s 674us/sample - loss: 0.0212 - accuracy: 0.9942 - val_loss: 0.0303 - val_accuracy: 0.9922\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 41s 682us/sample - loss: 0.0204 - accuracy: 0.9943 - val_loss: 0.0346 - val_accuracy: 0.9918\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 41s 676us/sample - loss: 0.0179 - accuracy: 0.9949 - val_loss: 0.0223 - val_accuracy: 0.9939\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 38s 641us/sample - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0246 - val_accuracy: 0.9928\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 37s 614us/sample - loss: 0.0189 - accuracy: 0.9946 - val_loss: 0.0272 - val_accuracy: 0.9934\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 37s 613us/sample - loss: 0.0173 - accuracy: 0.9952 - val_loss: 0.0236 - val_accuracy: 0.9930\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 37s 619us/sample - loss: 0.0157 - accuracy: 0.9957 - val_loss: 0.0231 - val_accuracy: 0.9945\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 37s 617us/sample - loss: 0.0168 - accuracy: 0.9952 - val_loss: 0.0231 - val_accuracy: 0.9935\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 37s 617us/sample - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.0270 - val_accuracy: 0.9931\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 38s 628us/sample - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.0275 - val_accuracy: 0.9942\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 37s 621us/sample - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.0290 - val_accuracy: 0.9930\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 38s 632us/sample - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.0361 - val_accuracy: 0.9907\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 35s 590us/sample - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.0242 - val_accuracy: 0.9946\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 35s 591us/sample - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.0304 - val_accuracy: 0.9927\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 35s 590us/sample - loss: 0.0109 - accuracy: 0.9968 - val_loss: 0.0360 - val_accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ed0c351d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 224us/sample - loss: 0.0360 - accuracy: 0.9915\n",
      "Score is : 0.03601376320495292\n",
      "Accuracy : 0.9915\n"
     ]
    }
   ],
   "source": [
    "score ,acc = model.evaluate(x_test,y_test)\n",
    "print(\"Score is :\",score)\n",
    "print(\"Accuracy :\",acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our model\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"models.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model,'/home/ubuntu/love/keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
